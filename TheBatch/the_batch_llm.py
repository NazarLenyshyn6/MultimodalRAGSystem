from typing import List
from dataclasses import dataclass, field

import pydantic
from langchain.prompts import PromptTemplate
from langchain.llms import Ollama

from TheBatch.the_batch_vectorestore_pipeline import the_batch_vectorestore
from TheBatch.the_batch_configs import the_batch_prompt_template
from TheBatch import the_batch_exceptions
from VectorStore.base_vector_store import VectorStoreI
from LLM.rag_llm import RAGLLMI
from LLM.rag_llm import OllamaRAGLLM
from Schema.schema import ImageDocument
from Internals.logger import logger

@dataclass
class TheBatchLLMResponse:
    """ Data transfer object representing the response from TheBatchLLM.

    Attributes:
        question (str): The original user query.
        text_response (str): The textual answer generated by the model.
        image_response (List[ImageDocument]): A list of ImageDocument instances
            relevant to the query.
    """
    question: str
    text_response: str = field(repr=False)
    image_response: List[ImageDocument] = field(repr=False)


class TheBatchLLM(pydantic.BaseModel):
    """
    Wrapper for a multimodal retrieval-augmented generation language model using Ollama.

    This class integrates:
        - An underlying Ollama LLM model.
        - A prompt template to format user queries.
        - A vector store for document retrieval.
        - A retrieval-augmented generation LLM (RAGLLM) instance for enhanced querying.

    Attributes:
        model (Ollama): The underlying language model instance (default: llama3.2).
        prompt_template (PromptTemplate): Template to format prompts for the model.
        vectorstore (VectorStoreI): Vector store interface for retrieving relevant documents.
        rag_llm (OllamaRAGLLM): RAG LLM instance, automatically initialized if None.

    Raises:
        THEBatchLLMInitializationError: If TheBatchLLM initialization fails.
    """
    model_config = pydantic.ConfigDict(arbitrary_types_allowed=True)
    model: Ollama = pydantic.Field(default=Ollama(model="llama3.2"), repr=False)
    prompt_template: PromptTemplate = pydantic.Field(repr=False)
    vectorstore: VectorStoreI
    rag_llm: OllamaRAGLLM = pydantic.Field(default=None)

    def model_post_init(self, context):
        try:
            logger.info("TheBatchLLM initialization")
            if self.rag_llm is None:
                self.rag_llm = OllamaRAGLLM(model=self.model,
                                            prompt_template=self.prompt_template,
                                            vectorstore=self.vectorstore)
            logger.info("TheBatchLLM initialization done successfuly.")
        except Exception as e:
            msg = "TheBatchLLM initialization failed."
            logger.exception(msg)
            raise the_batch_exceptions.THEBatchLLMInitializationError(msg) from e

    def query(self,
              user_query: str, 
              k: int = 5
              ) -> TheBatchLLMResponse:
        """
        Process a user query using retrieval augmented generation.

        This method:
            - Queries the RAG LLM to get the text response and relevant documents.
            - Extracts image documents from the retrieved relevant documents.
            - Returns a structured TheBatchLLMResponse containing the question,
              textual answer, and any image documents.

        Args:
            user_query (str): The input question from the user.
            k (int, optional): Number of relevant documents to retrieve (default is 5).

        Raises:
            TheBatchLLMAnswerGenerationError: If TheBatchLLM fails to answer user query.

        Returns:
            TheBatchLLMResponse: The complete response containing the question,
                                 generated text answer, and list of image documents.
        """
        try:
            logger.info("TheBatchLLM processing user query: %s", user_query)
            rag_llm_response = self.rag_llm.query(user_query=user_query, k=k)
            text_response = rag_llm_response.llm_resopnse
            image_response = [document for document in rag_llm_response.relevant_docs if isinstance(document, ImageDocument)]
            the_batch_response = TheBatchLLMResponse(question=user_query,
                                                    text_response=text_response,
                                                    image_response=image_response)
            logger.info("TheBatchLLM successfully processed user query: %s", user_query)
            return the_batch_response
        except Exception as e:
            msg = f"THEBatchLLM failed to answer user query: {user_query}."
            logger.exception(msg)
            raise the_batch_exceptions.THEBatchLLMAnswerGenerationError(msg) from e


